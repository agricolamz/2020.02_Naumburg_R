---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Text manipulations {#texts}

I will still use a lot of `tidyverse`:

```{r, message=FALSE}
library(tidyverse)
```

## `read_lines()`

If you want to read text in R you can use the `read_lines()` function:

```{r}
tajemnica_baskerville <- read_lines("https://raw.githubusercontent.com/agricolamz/2020.02_Naumburg_R/master/data/tajemnica_baskerville.txt")
```

As a result you will get a vector with characters. It is easy to convert it to dataframe:

```{r}
tibble(text = tajemnica_baskerville)
```

## `gutenbergr`
The `gutenbergr` package is an API for a very old [project Gutenberg](http://www.gutenberg.org/), that is a library of over 60,000 free eBooks. 

```{r}
library(gutenbergr)
```

The most important part of this package is the `gutenberg_metadata` dataset -- that is a catalogue of everything in the Gutenberg library.

```{r}
str(gutenberg_metadata)
```

How many languages are presented in the Gutenberg library?

```{r}
gutenberg_metadata %>% 
  count(language, sort = TRUE)
```

How many authors are available?

```{r}
gutenberg_metadata %>% 
  count(author, sort = TRUE)
```

How many Polish texts are available?

```{r}
gutenberg_metadata %>% 
  filter(language == "pl") %>% 
  count(author, sort = TRUE)
```

```{block, type = "rmdtask"}
Whose texts are the most numerous in the German part of the Gutenberg library? Put his/her last name in the form.
```

```{r, results='asis', echo = FALSE, eval = knitr::is_html_output()}
library(checkdown)

checkdown::autocheck_question(question_id = "5_1", answer =  "Goethe", wrong = "Without umlaut, please")
```

Let's have a look at Mickiewicz's texts in the Polish part of the Gutenberg library:

```{r}
gutenberg_metadata %>% 
  filter(author == "Mickiewicz, Adam",
         language == "pl")
```

Let's download Adam Mickiewicz's sonnets:

```{r gutenberg_1, cache=TRUE}
text <- gutenberg_download(27081)
```

```{r, eval = knitr::is_html_output()}
text
```


It is possible to use multiple ids. Let's also download some poems by [A. Mickiewicz (1798--1855)](https://en.wikipedia.org/wiki/Adam_Mickiewicz), [J. Kochanowski (1530--1584)](https://en.wikipedia.org/wiki/Jan_Kochanowski), [Z. Krasinski (1812--1859)](https://en.wikipedia.org/wiki/Zygmunt_Krasi%C5%84ski), and [A. Oppman (1867--1931)](https://en.wikipedia.org/wiki/Artur_Oppman):

```{r gutenberg_2, cache=TRUE}
texts <- gutenberg_download(c(27081, 27871, 28009, 27208))
```

```{r, eval = knitr::is_html_output()}
texts
```


Be aware:

* texts could include something from the real book: introduction or last word written by other people, publication details, etc.
* texts could be stored with the wrong encoding;
* texts could be stored with normalised orthography (e. g. Kochanowski, look at rows 99 and 100);

```{r}
texts %>% 
  filter(gutenberg_id == 27208)
```

* there are a lot of empty characters;
* and probably a lot of other problems.

I annotated those texts:

```{r}
texts <- read_csv("https://raw.githubusercontent.com/agricolamz/2020.02_Naumburg_R/master/data/mickiewicz_kochanowski_krasinski_oppman.csv")
```

Now it is possible to remove some non-important lines:

```{r}
texts %>% 
  filter(title != "remove") ->
  texts
```

```{r, eval = knitr::is_html_output()}
texts
```

```{block, type = "rmdtask"}
Calculate how many rows per author we do have in our dataset. Who has the largest amount?
```

```{r, results='asis', echo = FALSE, eval = knitr::is_html_output()}
checkdown::autocheck_question(question_id = "5_2", answer =  "Krasiński", wrong = "you can copy ń from here")
```

## `tidytext`

The `tidytext` [@silge17] (this book is available [online](https://www.tidytextmining.com/)) allows you to work with texts in tidy ideology, that makes it easier to manipulate, summarise, and visualise the characteristics of texts easily and integrate natural language processing tools (sentiment analysis, tf-idf metric, n-gram analysis, topic modeling etc.).

```{r}
library(tidytext)

texts %>% 
  unnest_tokens(output = "word", input = text)
```


```{r}
texts %>% 
  unnest_tokens(output = "word", input = text) %>% 
  group_by(author) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  coord_flip()+
  facet_wrap(~author, scales = "free")
```

As you see the sorting is bad. Sorting within different facets is possible with the `reorder_within()` function:

```{r}
texts %>% 
  unnest_tokens(output = "word", input = text) %>% 
  group_by(author) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>% 
  ggplot(aes(reorder_within(x = word, by = n, within = author), n))+
  geom_col()+
  coord_flip()+
  facet_wrap(~author, scales = "free")
```

In order to remove the author name you also need to add `scale_x_reordered()` layer to your ggplot:

```{r}
texts %>% 
  unnest_tokens(output = "word", input = text) %>% 
  group_by(author) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>% 
  ggplot(aes(reorder_within(word, n, author), n))+
  geom_col()+
  scale_x_reordered()+
  coord_flip()+
  facet_wrap(~author, scales = "free")
```

Often in text analysis, it is useful to remove stop words. Stop words are frequent words that mostly contain grammatic information. I will use a polish stopword list from this [repository](https://github.com/stopwords-iso/stopwords-pl), but you can use any other list or modify the existing one.

```{r}
stopwords <- read_lines("https://raw.githubusercontent.com/stopwords-iso/stopwords-pl/master/stopwords-pl.txt")
stopwords
```

So now we are ready to remove stopwords using the `antijoin()` function:

```{r}
texts %>% 
  unnest_tokens(output = "word", input = text) %>% 
  anti_join(tibble(word = stopwords)) %>% # here is the stopwords removal
  group_by(author) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>% 
  ggplot(aes(reorder_within(word, n, author), n))+
  geom_col()+
  scale_x_reordered()+
  coord_flip()+
  facet_wrap(~author, scales = "free")
```

It is also possible to analyse bigrams

```{r}
texts %>% 
  unnest_tokens(output = "bigrams", input = text, token = "ngrams", n = 2) %>% 
  # separate into two seperate columns each part of bigram
  separate(bigrams, into = c("word_1", "word_2"), sep = " ") %>%
  # filter out those that have stopwords
  anti_join(tibble(word_1 = stopwords)) %>% 
  anti_join(tibble(word_2 = stopwords)) %>% 
  # merge separate columns into one
  mutate(bigrams = str_c(word_1, word_2, sep = " ")) %>% 
  group_by(author) %>% 
  count(bigrams) %>% 
  top_n(4) %>% 
  ggplot(aes(reorder_within(bigrams, n, author), n))+
  geom_col()+
  scale_x_reordered()+
  coord_flip()+
  facet_wrap(~author, scales = "free")
```

Since our corpora for each author is really small we can't see much (e. g. Mickiewicz no repetitions). If the text will be longer (e. g. long novels), you will be able to get the most important. Lets analyse "Tajemnicę Baskerville'ów":

```{r tajemnica, cache=TRUE}
tajemnica <- gutenberg_download(34079)

tajemnica %>% 
  unnest_tokens(output = "bigrams", input = text, token = "ngrams", n = 2) %>% 
  # separate into two seperate columns each part of bigram
  separate(bigrams, into = c("word_1", "word_2"), sep = " ") %>%
  # filter out those that have stopwords
  anti_join(tibble(word_1 = stopwords)) %>% 
  anti_join(tibble(word_2 = stopwords)) %>% 
  # merge separate columns into one
  mutate(bigrams = str_c(word_1, word_2, sep = " ")) %>% 
  count(bigrams, sort = TRUE) %>% 
  top_n(20) %>% 
  ggplot(aes(fct_reorder(bigrams, n), n))+
  geom_col()+
  coord_flip()
```

```{block, type = "rmdtask"}
Analyse "Pan Tadeusz Czyli Ostatni Zajazd na Litwie" by A. Mickiewicz. What is the most frequent bigram in this text (remove stopwords)?
```

```{r, include=FALSE, eval=FALSE}
gutenberg_download(31536) %>% 
  unnest_tokens(output = "bigrams", input = text, token = "ngrams", n = 2) %>% 
  # separate into two seperate columns each part of the bigram
  separate(bigrams, into = c("word_1", "word_2"), sep = " ") %>%
  # filter out those that have stopwords
  anti_join(tibble(word_1 = stopwords)) %>% 
  anti_join(tibble(word_2 = stopwords)) %>% 
  # merge separate columns into one
  mutate(bigrams = str_c(word_1, word_2, sep = " ")) %>% 
  count(bigrams, sort = TRUE) %>% 
  top_n(5)
```

```{r, results='asis', echo = FALSE, eval = knitr::is_html_output()}
checkdown::autocheck_question(question_id = "5_3", answer =  "s tyłu", wrong = "you can copy ł from here")
```

## `udpipe`

The `udpipe` package gives you the ability to get lemmatisation, morphological and syntactic analysis for multiple languages. A tutorial and a list of availible languages can be found [here](https://bnosac.github.io/udpipe/docs/doc1.html).

All models are long to download:
```{r}
library(udpipe)
```

```{r, eval=FALSE}
udpipe_download_model(language = "polish-pdb")
```
    
Texts for udpipe analyser should have variables named `text` and `doc_id`:

```{r}
texts %>% 
  mutate(doc_id = str_c(author, "_", title)) ->
  texts_for_udpipe
```

```{r, eval = knitr::is_html_output()}
texts_for_udpipe
```

After you downloaded a model and created correct dataframe it is possible to analyse our texts:

```{r, eval = FALSE}
texts_parsed <- udpipe(x = texts_for_udpipe, 
                       object = udpipe_load_model("polish-pdb-ud-2.4-190531.udpipe"))
texts_parsed
```

```{r udpipe, echo=FALSE, cache= TRUE}
texts_parsed <- udpipe(x = texts_for_udpipe, 
                       object = udpipe_load_model("data/polish-pdb-ud-2.4-190531.udpipe"))
```

```{r, eval = knitr::is_html_output()}
texts_parsed
```


```{block, type = "rmdtask"}
What is the most frequent part of speech (`upos` variable) in our corpora according to the model?
```
    
```{r, results='asis', echo = FALSE, eval = knitr::is_html_output()}
checkdown::autocheck_question(question_id = "5_4", answer =  "NOUN")
```

## `stylo`
[The `stylo` package](https://github.com/computationalstylistics/stylo) [@eder16] is a package for computational stylistics, authorship attribution, etc.


First of all it is possible to use pure frequencies as a features for clustarisation:

```{r}
library(stylo)

texts %>% 
  mutate(doc_id = str_c(author, title, sep = "_")) %>% 
  unnest_tokens(text, output = "word") %>% 
  count(doc_id, word) %>% 
  group_by(doc_id) %>% 
  mutate(ratio = n/sum(n)) %>% 
  pivot_wider(names_from = doc_id, values_from = ratio, values_fill = list(ratio = 0)) %>% 
  select(-word, -n) ->
  for_stylo
```

```{r, eval = knitr::is_html_output()}
for_stylo
```


Then you can run a clustarisation analysis.

```{r, fig.height=12}
stylo(parsed.corpus = for_stylo, gui = FALSE, analysis.type = "CA")
```

You can run with argumment `gui = TRUE`, then you will see a graphical interface.

It is also possible to use lemmatized via `udpipe` representation.

```{r, fig.height=12}
texts_parsed %>% 
  count(doc_id, lemma) %>% 
  group_by(doc_id) %>% 
  mutate(ratio = n/sum(n)) %>% 
  pivot_wider(names_from = doc_id, values_from = ratio, values_fill = list(ratio = 0)) %>% 
  select(-lemma, -n) ->
  for_stylo_2

stylo(parsed.corpus = for_stylo_2, gui = FALSE, analysis.type = "CA")
```

Authorship Verification Classifier:

```{r}
texts %>% 
  mutate(author = ifelse(title == "Sonety I.", "test_mickiewicz", author),
         author = ifelse(title == "Sonety VI.", "test_mickiewicz", author),
         author = ifelse(title == "Sonety XX.", "test_mickiewicz", author),
         author = ifelse(title == "Sonety krymskie XV.", "test_mickiewicz", author),
         author = ifelse(title == "Sonety krymskie III.", "test_mickiewicz", author),
         author = ifelse(title == "THREN XIX.", "test_kochanowski", author),
         author = ifelse(title == "Niewymarzona, a cudowna.", "test_krasinski", author),
         author = ifelse(title == "OWCZAREK.", "test_oppman", author),
         author = ifelse(title == "EMIGRANT.", "test_oppman", author)) %>% 
  unnest_tokens(output = "word", input = text) %>% 
  count(author, word, sort = TRUE) %>% 
  filter(n > 1) %>% 
  group_by(author) %>% 
  mutate(ratio = n/sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = ratio, values_fill = list(ratio = 0)) %>% 
  as.data.frame() -> # stylo package doesn't work with tibble
  for_imposter
```

```{r, eval = knitr::is_html_output()}
for_imposter
```


Lets choose Krasiński:
```{r krasinski, cache = TRUE}
imposters(reference.set = for_imposter[-c(5:8), -c(1:2)],
# for some reason this function demands minimum 2 rows in dataframe (contra docs)
          test = for_imposter[c(5, 5), -c(1:2)],
          classes.reference.set = for_imposter[-c(5:8), 1])
```

Lets choose Kochanowski:
```{r kochanowski, cache = TRUE}
imposters(reference.set = for_imposter[-c(5:8), -c(1:2)],
# for some reason this function demands minimum 2 rows in dataframe (contra docs)
          test = for_imposter[c(6, 6), -c(1:2)],
          classes.reference.set = for_imposter[-c(5:8), 1])
```

Lets choose Mickiewicz:

```{r mickiewicz, cache = TRUE}
imposters(reference.set = for_imposter[-c(5:8), -c(1:2)],
# for some reason this function demands minimum 2 rows in dataframe (contra docs) 
          test = for_imposter[c(7, 7), -c(1:2)],
          classes.reference.set = for_imposter[-c(5:8), 1])
```

Lets choose Oppman:

```{r oppman, cache = TRUE}
imposters(reference.set = for_imposter[-c(5:8), -c(1:2)],
# for some reason this function demands minimum 2 rows in dataframe (contra docs)
          test = for_imposter[c(8, 8), -c(1:2)],
          classes.reference.set = for_imposter[-c(5:8), 1])
```

As you see, this algorithm did not work on this verse dataset, but in docs (`?imposters()`) you can find an example of analysis of the novel "Cuckoo's Calling" by a mysterious Robert Galbraith that turned out to be J.K. Rowling.
